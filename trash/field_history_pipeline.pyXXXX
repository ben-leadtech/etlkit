"""
Road-test the ETL framework
"""
import os
import pandas as pd

from etlkit.etlkit import logging
#from etlkit.loaders import BigQueryLoader
from etlkit.etlkit.extractors import SalesforceExtractor, MultiExtractor
from etlkit.etlkit.templates import TemplatePipeline, ExtractTemplate, \
	TransformTemplate, LoadTemplate
from etlkit.etlkit.containers import Data, Config

import dotenv
dotenv.load_dotenv()
ENVIRONMENT = os.getenv('ENVIRONMENT')





#================================================================================#
class PPDExtractor(ExtractTemplate):

	def run(self) -> Data:
		mex = MultiExtractor(async_extract=True)

		# Create the jobs
		mex.create_job(
			extractor=SalesforceExtractor(bulk=True),
			name='opps',
			query=f"""
	SELECT Id, Partner_Lead_ID__c, CreatedDate
	FROM Opportunity
	WHERE LastModifiedDate >= {self.config.min_date}
	AND (NOT Product_Name__c LIKE '%Test%')
	AND (NOT partner_display_name__c LIKE '%Lead Tech%')
	AND (NOT partner_display_name__c LIKE '%Duplicate%')
	AND (NOT Account_Name_Formula__c LIKE '%Sandbox%')
		""")

		mex.create_job(SalesforceExtractor(bulk=True),
			name='fh',
			query=f"""
		SELECT Id, OpportunityId, CreatedDate, OldValue, NewValue
		FROM OpportunityFieldHistory
		WHERE CreatedDate >= {self.config.min_date}
		AND Field = 'Partner_Performance_Disposition__c'
		""")

		data = mex.run()
		return data
#================================================================================#




#================================================================================#
class PPDTransformer(TransformTemplate):
	"""
	Functions for the field history ELT pipeline
	"""

	#___________________________________________________________________________#
	def ohe_fh(self):
		"""
		One-hot encode the field history data, with the field names as the columns and
		the values as their timestamps
		"""
		logging.info("One-hot encoding the field history data...")
		df: pd.DataFrame = self.data.dataframes['df_fh'].copy()

		# OHE the field history data
		df_ohe = pd.get_dummies(df['NewValue']).astype(int)
		df_tot = pd.concat([df, df_ohe], axis=1)

		# Replace the '1's with the timestamps, then determine the number of days
		# since the CreatedDate
		for col in df_ohe.columns:
			df_tot[col] = pd.to_datetime(df_tot[col] * df_tot['CreatedDate'])


		# Drop the columns we don't need
		cols_to_drop = ['NewValue', 'OldValue', 'CreatedDate', 'Id','']
		for col in cols_to_drop:
			if col in df_tot.columns:
				df_tot.drop(col, axis=1, inplace=True)


		# Replace any non alpha numeric / "_- "" characters in the column names
		df_tot.columns = df_tot.columns.\
											str.replace('[^a-zA-Z0-9_-]', ' ', regex=True).\
											str.replace('  ',' -')

		# Group by OpportunityId. We aggregate as 'min', as there should only be
		# one timestamp per column.
		df_tot.rename(columns={'OpportunityId':'Id'}, inplace=True)
		df_g = df_tot.groupby('Id').min().reset_index()

		# Store in self
		self.data.dataframes['df_final'] = df_g


	#___________________________________________________________________________#
	def merge_opps(self):
		"""
		Merge with Opps, using the ID as the key, to get Partner_Lead_ID.
		"""
		df: pd.DataFrame       = self.data.dataframes['df_opps'].copy()
		df_final: pd.DataFrame = self.data.dataframes['df_final'].copy()

		df.rename(columns={'CreatedDate':'Created_Date',
											'Partner_Lead_ID__c': 'Partner_Lead_ID'}, inplace=True)
		df['Created_Date'] = pd.to_datetime(df['Created_Date'])
		df['Unique_ID'] = (df['Created_Date'].astype('string') + '_' +
												df['Id'].astype('string')).str.replace(' ', '')

		logging.info("Merging the field history data with the opportunity data...")
		df = df.merge(df_final, on='Id', how='inner')

		# Replace timestamps in the OHE fields with the number of days since
		# 'Created_Date'.
		logging.info("Replacing timestamps with the number of days since 'Created_Date'...")
		df = self._ppd_time_delta(df)

		self.data.dataframes['df_final'] = df_final


	#___________________________________________________________________________#
	def _ppd_time_delta(self, df:pd.DataFrame) -> pd.DataFrame:
		"""
		Replace timestamps in the OHE fields with the number of days since
		'Created_Date'.
		"""

		# Find the OHE columns. These are cols with timestamp dtypes but are
		# not called 'Created_Date'
		ohe_cols = []
		for col in df.columns:
			if 'datetime' in str(df[col].dtype) and col != 'Created_Date':
				df[col] = (pd.to_datetime(df[col]) - pd.to_datetime(df['Created_Date'])).\
					dt.days.astype('float')
				ohe_cols.append(col)

		# Drop those rows where all the OHE columns are null
		df.dropna(subset=ohe_cols, how='all', inplace=True)

		return df

	#___________________________________________________________________________#
	def add_uid(self):
		"""
		Add a unique ID to the dataframe
		"""
		df: pd.DataFrame = self.data.dataframes['df_final'].copy()
		df['Unique_ID'] = df['Id'] # We just copy the Id for now
		self.data.dataframes['df_final'] = df


	#___________________________________________________________________________#
	def run(self, data: Data) -> pd.DataFrame:
		"""
		Run the functions
		"""
		# First: is the data what we're expecting?
		self.test_inputs(data)

		# Run the functions
		self.data = data
		self.ohe_fh()
		self.merge_opps()
		self.add_uid()

		return self.data.dataframes['df_final']

#================================================================================#



#================================================================================#
# class PPDLoader(LoadTemplate):
# 	def load(self, df: pd.DataFrame, config: Config) -> None:
# 		_ = BigQueryLoader()(df, config)

# 	def run(self, df: pd.DataFrame, config: Config) -> None:
# 		self.load(df, config)
#================================================================================#




#================================================================================#
def main_v2():
	config = Config(table_name='ppd_history',
									dataset_name=f'{ENVIRONMENT}_published',
									update_mode=True)

	pipeline = TemplatePipeline(
							extractor=PPDExtractor,
							transformer=PPDTransformer,
							#loader=PPDLoader,
							config=config)

	pipeline()



#================================================================================#
if __name__ == '__main__':
	main_v2()
#================================================================================#
